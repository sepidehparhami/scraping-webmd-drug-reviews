{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nimport numpy as np\nimport pandas as pd\nimport regex as re","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:20:40.143773Z","iopub.execute_input":"2023-12-02T21:20:40.144176Z","iopub.status.idle":"2023-12-02T21:20:40.149720Z","shell.execute_reply.started":"2023-12-02T21:20:40.144140Z","shell.execute_reply":"2023-12-02T21:20:40.148970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# populate data frame by parsing 1 page of document\ndef parse_reviews_page(reviews_html, drug_name, reviews_df):\n    for i, review in enumerate(reviews_html):\n        to_append = pd.DataFrame([pd.Series([None]*len(cols), index=cols)])\n\n        to_append['drug_name'] = drug_name\n        to_append['date'] = re.findall(r'\\d+/\\d+/\\d+', review.find('div', class_='date').text)[0]\n\n        details = review.find('div', class_='details').text\n\n        age_match = re.findall(r'(?<=\\|\\s+)\\d+-\\d+', details)\n        to_append['age'] = age_match[0] if len(age_match) > 0 else np.nan\n\n        gender_match = re.findall(r'(?<=\\|\\s+)Male|Female', details)\n        to_append['gender'] = gender_match[0] if len(gender_match) > 0 else np.nan\n\n        time_match = re.findall(r'(?<=On\\smedication\\sfor\\s)\\w+(?:\\s\\w+)*', details)\n        to_append['time_on_drug'] = time_match[0] if len(time_match) > 0 else np.nan\n\n        type_match = re.findall(r'(?<=\\|\\s+)\\w+(?:\\s\\w+)*(?=\\s+$)', details)\n        to_append['reviewer_type'] = type_match[0] if len(type_match) > 0 else np.nan    \n\n        condition_element = review.find('strong', class_='condition')\n        condition_listed = condition_element is not None\n        if condition_listed:\n            # TODO: be able to match ''\"Change of Life\" Signs' condition\n            condition_match = re.findall(r'(?<=Condition:\\s)\\w+(?:\\s\\w+)*', condition_element.text)\n        to_append['condition'] = condition_match[0] if (condition_listed and len(condition_match) > 0) else np.nan\n\n        rating_overall_line = review.find('div', class_='overall-rating').strong.text\n        to_append['rating_overall'] = re.findall(r'\\d+.\\d+', rating_overall_line)\n\n        rating_categories = review.find('div', class_='categories').find_all('section')\n\n        for j, cat in enumerate(['effectiveness', 'ease_of_use', 'satisfaction']):\n            div = rating_categories[j].find('div', class_='webmd-rate on-mobile')\n            to_append[f'rating_{cat}'] = int(div.get('aria-valuenow'))\n    \n        text_line = review.find('p', class_='description-text')\n        to_append['text'] = text_line.text if text_line is not None else np.nan\n\n        reviews_df = pd.concat([reviews_df, to_append], ignore_index=True)\n        \n    return reviews_df","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:47:17.378465Z","iopub.execute_input":"2023-12-02T21:47:17.378849Z","iopub.status.idle":"2023-12-02T21:47:17.391801Z","shell.execute_reply.started":"2023-12-02T21:47:17.378820Z","shell.execute_reply":"2023-12-02T21:47:17.390422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# crawl page by page and save reviews from each page into reviews_df\ndef crawl_reviews_pages(reviews_df):\n    curr_page = 0\n    pages_left = True\n    while pages_left:\n        curr_page += 1\n\n        if (curr_page % 15) == 0:\n            print(f'Scraping page {curr_page} of {last_page}')\n\n        curr_url = review_url + f'&page={curr_page}'\n        response = requests.get(curr_url, headers=headers).content\n        soup = BeautifulSoup(response, 'lxml')\n        reviews_page_html = soup.find_all('div', class_='review-details') # get elements that hold each review\n        pages = soup.find('ul', class_='pagination')\n        last_page = int(pages.find_all('li', class_='page-item')[-1].text.strip())\n\n        page_title = soup.title.text\n        drug_name = re.findall('\\w+(?:\\s\\w+)*(?=\\sReviews)', page_title)[0]\n\n        reviews_df = parse_reviews_page(reviews_page_html, drug_name, reviews_df)\n\n        if curr_page >= last_page:\n            pages_left = False\n            \n    return reviews_df","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:47:18.102373Z","iopub.execute_input":"2023-12-02T21:47:18.102750Z","iopub.status.idle":"2023-12-02T21:47:18.110863Z","shell.execute_reply.started":"2023-12-02T21:47:18.102704Z","shell.execute_reply":"2023-12-02T21:47:18.109318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://bar.rady.ucsd.edu/Web_Scraping.html\n\nfrom bs4 import BeautifulSoup\n\n# need to spoof a browser in order to not get blocked when making request\nheaders = requests.utils.default_headers()\nheaders.update({\n    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n})\n\n# list of depression drugs from WebMD\nurl = 'https://www.webmd.com/depression/depression-medications-antidepressants'\nprint(url)\nresponse = requests.get(url, headers=headers).content\nsoup = BeautifulSoup(response, 'lxml')\ndrugs_section = soup.find('div', class_='article-page active-page')","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:47:18.557846Z","iopub.execute_input":"2023-12-02T21:47:18.558234Z","iopub.status.idle":"2023-12-02T21:47:18.807479Z","shell.execute_reply.started":"2023-12-02T21:47:18.558203Z","shell.execute_reply":"2023-12-02T21:47:18.806088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make empty data frame to contain values and full text from each review\ncols = ['drug_name',\n       'date',\n       'age',\n       'gender',\n       'time_on_drug',\n       'reviewer_type',\n       'condition',\n       'rating_overall',\n       'rating_effectiveness',\n       'rating_ease_of_use',\n       'rating_satisfaction',\n       'text']\n\nreviews_df = pd.DataFrame(columns=cols, index=[])\n\n# TODO: change parsing so that it looks for reviews.webmd.com in case ordering of tabs changes\nfor drug in drugs_section.find_all('p'):\n    link = drug.a.get('href')\n    response = requests.get(link, headers=headers).content\n    drug_page = BeautifulSoup(response, 'lxml')\n    \n    drug_review_element = drug_page.find('ul', class_='auto-tabs').find_all('li')[-1] # get the data for the last tab, Reviews\n    review_url = drug_review_element.a.get('href')\n    print(review_url)\n    reviews_df = crawl_reviews_pages(reviews_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T21:47:19.238362Z","iopub.execute_input":"2023-12-02T21:47:19.238782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.to_csv('psychiatric_drug_webmd_reviews.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}